{"cells":[{"metadata":{"id":"P0OKyqBic1Xb"},"cell_type":"markdown","source":"# Overview of the competition (as presented on [kaggle](https://www.kaggle.com/c/siim-isic-melanoma-classification/overview))\n\nSkin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective.\n\nIn this competition, you’ll identify melanoma in images of skin lesions. In particular, you’ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.\n\nMelanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people."},{"metadata":{"id":"ldpJhIbWc1Xc"},"cell_type":"markdown","source":"# Data\n\nThe dataset contains 33,126 dermoscopic training images of unique benign and malignant skin lesions from over 2,000 patients. For each image, we are provided with the following information:\n\n* dermoscopic image;\n* patient_id - unique patient identifier;\n* sex - the sex of the patient (when unknown, will be blank);\n* age_approx - approximate patient age at time of imaging;\n* anatom_site_general_challenge - location of imaged site;\n* diagnosis - detailed diagnosis information (train only);\n* benign_malignant - indicator of malignancy of imaged lesion;\n* target - binarized version of the target variable;\n\nWe will use an external datasource that processed the given dataset in a very convenient format to use for Machine Learning training, as described [here](https://www.kaggle.com/cdeotte/melanoma-256x256). Our input images have size 256x256. Several other size options are available.\n\n\n## What are we predicting?\n\nWe are predicting a binary target for each image. Your model should predict the probability (floating point) between 0.0 and 1.0 that the lesion in the image is malignant (the target). In the training data, train.csv, the value 0 denotes benign, and 1 indicates malignant.\n\n## How are predictions evaluated?\n\nSubmissions are evaluated on area under the ROC curve (AUC) between the predicted probability and the observed target. This is particularly useful here, as the data provided is highly imbalanced, with only 1.8% malignant images, and if we were to use accuracy as a metric then a model that always predicts benign will have accuracy 98.2%. The Area Under the ROC curve (AUC) is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive (see [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) for a proof of this interpretation)."},{"metadata":{"id":"O-4taoJUc1Xd"},"cell_type":"markdown","source":"# Machine Learning Algorithm for predicting melanoma"},{"metadata":{"id":"hkBMayMBc1Xd"},"cell_type":"markdown","source":"## Non-technical overview of the model"},{"metadata":{"id":"mnnuRN03c1Xe"},"cell_type":"markdown","source":"For a computer, images are simply an array of real numbers representing pixel intensities accross 3 channels (Red, Green and Blue). \n\n![](https://www.kdnuggets.com/wp-content/uploads/image-classification-cat-1.jpg)\n\n[Source of the image](https://www.kdnuggets.com/2017/08/convolutional-neural-networks-image-recognition.html)\n\nConvolutional Neural Networks (CNNs) are designed to process these arrays of numbers and extract *meaningful* interpretations of the images. These interpretations contain information such as locations of edges, or distinctive patterns and allow a CNN model to make a classification decision (in our case, is this benign or malign). For example, after going through a series of transformations as part of a model, the above cat image could have the one dimensional array representation\n\n$$A = [2.17, -0.44, 3.01, 4.002, -11]$$\n\nWhile the original input is also given as an array, slight variations of position, rotation, or contrast of an image can yield very different input arrays. It is thus essential for a CNN model to learn to account for these variations, so that the final array representation is robust and pictures of same entities end up having similar array representations.\n\nThe model learns simultaneously two things as part of the training: \n\n1. how to effectively represent the image as a one dimensional array;\n2. how to use that representation for making the final call;\n\nFor the melanoma classification, we will choose a model that was already trained to achieve high accuracy on Task 1. Specifically, we will download a pre-trained model that achieved high-accuracy when asked to discriminate accross thousands of classes of images, and we will fine-tune it to learn melanoma image embeddings by training it on our dataset. We also complement the model by giving it the task to learn from scratch useful representations for the other metadata features provided (sex, age, etc). As part of the training the model will thus perform the following steps:\n\n1. fine-tune a pre-trained image embedder;\n2. learn useful embeddings for the additional metadata features;\n3. combine the image and the metadata embeddings;\n4. use the final embedding to decide if the result is benign or malignant."},{"metadata":{"id":"TAhYkwRxc1Xf"},"cell_type":"markdown","source":"## Technical specs of the model\n\nWhile one can readily obtain great results by using only a powerful image classification model, we will create a model that also makes use of the additional metadata provided. \nFor this, we will combine a pretrained image model with an embedding of the additional metadata that we define ourselves. The steps are as follows:\n1. Download a pretrained image classifier that also performs very well on **transfer learning**;\n2. Remove the top (output) layer of the image model, so that we actually have a pretrained **image embedder**;\n3. Define embeddings for the rest of the metadata;\n4. Combine the image embedder with the metadata embeddings and train the resulting model on the provided data.\n\nThe model's architecture is illustrated below\n\n![model architecture](https://i.ibb.co/4mKk1Jb/model-with-metadata.png)\n\nFor Steps 1 and 2 above, we will use an EfficientNet Model. As described in their 2019 [paper](https://arxiv.org/pdf/1905.11946.pdf), this family of models achieved State-of-the-art performance on both image classification and transfer learning, while being significantly smaller (in number of parameters) and faster than best existing models. Moreover, the TensorFlow implementation of the model allows us to download the model with the pretrained weights on ImageNet and without the top layer, as desired.\n\nTo increase model's performance, in addition to creating a combined metadata + image model, the following techniques have been used:\n1. augment images using techniques such as cropping, rotation, shear;\n2. use several test time augmentation for images and average over the predictions to obtain a final prediction;\n3. use [label smoothing](https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06) and dropout for regularization;\n4. use a learning rate scheduler, which determines the size and evolution of learning rate during training.\n\nThe above listed steps for increasing model's performance were already provided in the public kernel https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once and what is presented below are merely small adaptations."},{"metadata":{"id":"x1b4rAyEc1Xg"},"cell_type":"markdown","source":"## Model's performance\n\nThe model below achieved 0.9224 AUC on the competition (for comparison, the first place achieved 0.9490). Since we didn't use a specified seed while training the model, your performance might differ slightly if reproducing the steps below, due to random initializations and Dropout choices that take place during training. Using the TPUs, the training time of the below algorithm is approximately one hour."},{"metadata":{"id":"XzP1VNUvc1Xg"},"cell_type":"markdown","source":"# Acknowledgements\n\nI would like to thank my friend Raluca Turc for working together with me on this competition.\nI would like to thank everyone that made their work public as part of the competition and allowed other participants like me to learn from their work. In particular, I would like to thank [Chris Deotte](https://www.kaggle.com/cdeotte) and the author of [this kernel](https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once) (user [AgentAuers](https://www.kaggle.com/agentauers)) for preparing the data pipeline in such a nice and easily adaptable way."},{"metadata":{"id":"BiRLm4m1c_g1"},"cell_type":"markdown","source":"# Reproducing the model"},{"metadata":{"id":"AVgJw70Qc1Xh"},"cell_type":"markdown","source":"## Instructions for running the cells below on [kaggle](http://www.kaggle.com)\n\n\nOn the RHS toolbar:\n1. Toggle on the \"Internet\" button.\n2. Import the \"melanoma-256x256\" dataset.\n3. Switch the accelerator to TPU."},{"metadata":{"id":"lM_GbTp5c1Xi"},"cell_type":"markdown","source":"## Installations & Imports"},{"metadata":{"trusted":true,"id":"AUxeGb_9c1Xi"},"cell_type":"code","source":"!pip install -q efficientnet","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"LsbyEDkyc1Xm"},"cell_type":"code","source":"import math\nimport os\nimport random\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nimport efficientnet.tfkeras as efn\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\n\nfrom kaggle_datasets import KaggleDatasets\n\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.preprocessing.image import random_rotation, random_shear, random_shift, random_zoom\nfrom tensorflow.keras.models import load_model\nfrom tensorflow import feature_column as fc\nfrom tensorflow.compat.v1.tpu.experimental import embedding_column as tpu_embedding_col","execution_count":null,"outputs":[]},{"metadata":{"id":"X9zViao0c1Xo"},"cell_type":"markdown","source":"## Data Pipeline"},{"metadata":{"id":"S3Ahf5uyc1Xp"},"cell_type":"markdown","source":"The pipeline below for loading tf records and training has been adapted from the kernel https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once"},{"metadata":{"id":"mUzSgZmzc1Xp"},"cell_type":"markdown","source":"## Load the external dataset"},{"metadata":{"trusted":true,"id":"7UsjNmdDc1Xq"},"cell_type":"code","source":"BASEPATH = \"../input/siim-isic-melanoma-classification\"\ndf_train = pd.read_csv(os.path.join(BASEPATH, 'train.csv'))\ndf_test  = pd.read_csv(os.path.join(BASEPATH, 'test.csv'))\ndf_sub   = pd.read_csv(os.path.join(BASEPATH, 'sample_submission.csv'))\n\nGCS_PATH    = KaggleDatasets().get_gcs_path('melanoma-256x256')\nfiles_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')))\nfiles_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')))\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{"id":"9lBVkDmwc1Xt"},"cell_type":"markdown","source":"## Configure the TPU"},{"metadata":{"trusted":true,"id":"lkfB9dRlc1Xt"},"cell_type":"code","source":"DEVICE = \"TPU\"\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{"id":"G6ssw8_Kc1Xw"},"cell_type":"markdown","source":"## Define the image loading pipeline"},{"metadata":{"id":"bOsKhCTxc1Xx"},"cell_type":"markdown","source":"### Define pipeline configuration"},{"metadata":{"trusted":true,"id":"RJyZoFrJc1Xx"},"cell_type":"code","source":"CFG = {\n    # image augmentation configs\n    'read_size': 256, \n    'crop_size': 250,\n    'net_size': 248,\n    'rot': 180.0,\n    'shr': 1.5,\n    'hzoom': 6.0,\n    'wzoom': 6.0,\n    'hshift': 6.0,\n    'wshift': 6.0,\n    \n    # learning rate scheduler configs\n    'LR_START': 0.000003,\n    'LR_MAX': 0.000020,\n    'LR_MIN': 0.000001,\n    'LR_RAMPUP_EPOCHS': 5,\n    'LR_SUSTAIN_EPOCHS': 0,\n    'LR_EXP_DECAY': 0.8,\n    \n    # model configs\n    'model_name': 'EfficientNetB6',\n    'batch_size': 16,\n    'epochs': 50,\n    'optimizer': 'adam',\n    'label_smooth_fac': 0.01,\n    'FC_size': 256,\n    'tta_steps': 25} # test time augmentation","execution_count":null,"outputs":[]},{"metadata":{"id":"43rp-YBac1X0"},"cell_type":"markdown","source":"### Functions for reading the tfrecords"},{"metadata":{"trusted":true,"id":"3r1u-jhhc1X1"},"cell_type":"code","source":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'sex': tf.io.FixedLenFeature([], tf.int64),\n        'age_approx': tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n        # These features are available for training only.\n        #'diagnosis': tf.io.FixedLenFeature([], tf.int64),\n        #'patient_id': tf.io.FixedLenFeature([], tf.int64),\n        'target': tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    target = example.pop('target')\n    return example, target\n\n\ndef read_unlabeled_tfrecord(example, return_image_names):\n    tfrec_format = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'sex': tf.io.FixedLenFeature([], tf.int64),\n        'age_approx': tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example, example['image_name'] if return_image_names else 0","execution_count":null,"outputs":[]},{"metadata":{"id":"wYcr0oWcc1X3"},"cell_type":"markdown","source":"## Function for image augmentations"},{"metadata":{"trusted":true,"id":"WKo97mNPc1X4"},"cell_type":"code","source":"def prepare_image(img, augment=True):    \n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [CFG['read_size'], CFG['read_size']])\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    if augment:\n        img = transform(img)\n        img = tf.image.random_crop(img, [CFG['crop_size'], CFG['crop_size'], 3])\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_hue(img, 0.01)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n\n    else:\n        img = tf.image.central_crop(img, CFG['crop_size'] / CFG['read_size'])\n                                   \n    img = tf.image.resize(img, [CFG['net_size'], CFG['net_size']])\n    img = tf.reshape(img, [CFG['net_size'], CFG['net_size'], 3])\n    return img\n\n\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear    = math.pi * shear    / 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n                               zero,            one/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\ndef transform(image):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = CFG[\"read_size\"]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = CFG['rot'] * tf.random.normal([1], dtype='float32')\n    shr = CFG['shr'] * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG['hzoom']\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG['wzoom']\n    h_shift = CFG['hshift'] * tf.random.normal([1], dtype='float32') \n    w_shift = CFG['wshift'] * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM, DIM,3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_pqiEmZlc1X6"},"cell_type":"code","source":"def prepare_image_in_example(example, augment):\n    example[\"image\"] = prepare_image(example[\"image\"], augment=augment)\n    return example\n\n\ndef remove_image_name(example):\n    example.pop('image_name', None)\n    return example\n\n\ndef get_dataset(files, augment=False, shuffle=False, repeat=False, \n                labeled=True, return_image_names=True,is_test=False):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(1024*8)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n                    num_parallel_calls=AUTO)     \n    \n    ds = ds.map(\n                lambda example, imgname_or_label: (prepare_image_in_example(example, augment=augment), imgname_or_label), \n                num_parallel_calls=AUTO)\n    \n    if is_test:\n        ds = ds.map(\n            lambda example, imgname_or_label: (remove_image_name(example), imgname_or_label))\n    \n    ds = ds.batch(CFG['batch_size'] * REPLICAS)\n    ds = ds.prefetch(AUTO)\n    \n    if is_test:\n        return ds\n\n    ds = ds.map(lambda img, label: (img, tuple([label])))\n    return ds\n","execution_count":null,"outputs":[]},{"metadata":{"id":"wCSse1oXc1X9"},"cell_type":"markdown","source":"## Create the dataset"},{"metadata":{"trusted":true,"id":"f17V6SQkc1X9"},"cell_type":"code","source":"ds_train = get_dataset(\n    files_train, augment=True, shuffle=True, repeat=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZkZ1voJwc1YA"},"cell_type":"markdown","source":"## Create the image embedder\n\n**Important note**: While we want to optimize for AUC, the AUC function is not differentiable, and practice has shown that a good indirect way for optimizing for AUC is to simply use Binary Cross Entropy. For more details, see [here](https://towardsdatascience.com/explicit-auc-maximization-70beef6db14e)."},{"metadata":{"trusted":true,"id":"ghuwmB5cc1YB"},"cell_type":"code","source":"def get_efficientnet_model():\n    \"\"\"Builds an EfficientNet model.\"\"\"\n    model_input = tf.keras.Input(shape=(CFG['net_size'], CFG['net_size'], 3), name='image')\n    dummy = tf.keras.layers.Lambda(lambda x:x)(model_input)    \n    outputs = []    \n\n    constructor = getattr(efn, CFG['model_name'])\n    x = constructor(include_top=False, weights='imagenet', \n                    input_shape=(CFG['net_size'], CFG['net_size'], 3), \n                    pooling='avg')(dummy)\n\n    outputs.append(x)\n    \n    model = tf.keras.Model(model_input, outputs, name=CFG['model_name'])\n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"G8Q0NEQxc1YD"},"cell_type":"code","source":"def compile_model(model_fn):\n    \"\"\"Compiles a model by calling model_fn().\"\"\"\n    with strategy.scope():\n        model = model_fn()\n        losses = tf.keras.losses.BinaryCrossentropy(label_smoothing = CFG['label_smooth_fac'])\n        \n        model.compile(\n            optimizer=CFG['optimizer'],\n            loss=losses,\n            experimental_run_tf_function=False,\n            metrics=[tf.keras.metrics.AUC(name='auc')])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"TX1ZBXl7c1YH"},"cell_type":"markdown","source":"## Define learning rate scheduler"},{"metadata":{"trusted":true,"id":"mR13YNzmc1YI"},"cell_type":"code","source":"def get_lr_callback():\n    lr_start = CFG['LR_START']\n    lr_max = CFG['LR_MAX'] * strategy.num_replicas_in_sync\n    lr_min = CFG['LR_MIN']\n    lr_ramp_ep = CFG['LR_RAMPUP_EPOCHS']\n    lr_sus_ep = CFG['LR_SUSTAIN_EPOCHS']\n    lr_decay = CFG['LR_EXP_DECAY']\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","execution_count":null,"outputs":[]},{"metadata":{"id":"8XlztnU5c1YK"},"cell_type":"markdown","source":"## Define the final model that also trains on metadata"},{"metadata":{"trusted":true,"id":"rxVpnRxzc1YL"},"cell_type":"code","source":"def get_efficientnet_with_metadata_model():\n    \"\"\"Defines a model that uses both the image and metadata like sex & age.\"\"\"\n\n    image_embedder = get_efficientnet_model()\n \n    metadata_inputs = {name: layers.Input(name=name, shape=(), dtype=\"int64\")\n                       for name in [\"sex\", \"age_approx\", \"anatom_site_general_challenge\"]}\n\n    all_inputs = {\"image\": image_embedder.input}\n    all_inputs.update(metadata_inputs)\n\n    sex = fc.categorical_column_with_identity(\"sex\", num_buckets=2)\n    age_in_years = fc.bucketized_column(fc.numeric_column(\"age_approx\"), boundaries=list(range(100)))\n    \n    ALL_SITES = ['head/neck', 'upper extremity', 'lower extremity', 'torso', 'palms/soles', 'oral/genital']\n    site = fc.categorical_column_with_identity(key=\"anatom_site_general_challenge\", num_buckets=len(ALL_SITES))\n    \n    feature_columns = {\n        # Single continuous value (either 0.0 or 1.0).\n        \"sex\": fc.numeric_column(\"sex\", dtype=tf.float32),\n        # One-hot vector of size = 100 (100 years)\n        \"year\": fc.indicator_column(age_in_years),\n        # Normalized age between 0 and 1; The categorical columns above fail to illustrate that e.g. 34yo is closer to\n        # 35yo than to 100yo, since all buckets are independent.\n        \"age_continuous\": fc.numeric_column(\"age_approx\", normalizer_fn=lambda age: tf.cast(age, tf.float32) / 100.0),\n        # One-hot vector with one entry per unique site.\n        \"anatom_site_general_challenge\": fc.indicator_column(site),\n    }\n    \n    if DEVICE != \"TPU\":\n        # One-hot vector of size = 200 (2 sexes x 100 years). Doesn't work on TPU (there's no kernel implementation)\n        feature_columns[\"sex_x_age\"] =  fc.indicator_column(fc.crossed_column([sex, age_in_years], hash_bucket_size=200))\n    \n    metadata_embedding = layers.DenseFeatures(feature_columns.values(), name=\"metadata\")(metadata_inputs)\n    hidden_layer_meta_1 = tf.keras.layers.Dense(256, activation='relu', name=\"hidden_layer_meta_1\")(metadata_embedding)\n    hidden_layer_meta_2 = tf.keras.layers.Dense(32, activation='relu', name='hidden_layer_meta_2')(hidden_layer_meta_1)\n\n    dropout = tf.keras.layers.Dropout(0.5)(image_embedder.output)\n    final_embedding = layers.concatenate([hidden_layer_meta_2, dropout])\n    hidden_layer = tf.keras.layers.Dense(CFG['FC_size'], name=\"hidden_layer\")(final_embedding)\n    output = tf.keras.layers.Dense(1, activation='sigmoid', name=\"probability\")(hidden_layer)\n    return tf.keras.Model(all_inputs, output)\n\nplot_model(get_efficientnet_with_metadata_model(), to_file='model_with_metadata.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"2dQHf_rjc1YO"},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true,"id":"9HvHDk2Lc1YO"},"cell_type":"code","source":"steps_train = count_data_items(files_train) / (CFG['batch_size'] * REPLICAS)\n\nmodel_with_metadata = compile_model(get_efficientnet_with_metadata_model)\nmodel_with_metadata.summary()\n\nmodel_with_metadata.fit(\n    ds_train,\n    verbose=1,\n    steps_per_epoch=steps_train,\n    epochs=CFG['epochs'],\n    callbacks=[get_lr_callback()]\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"S4Gqt_M4c1YR"},"cell_type":"markdown","source":"## Submission code"},{"metadata":{"trusted":true,"id":"e0MtQngbc1YR"},"cell_type":"code","source":"CFG['batch_size'] = 256\n\ncnt_test = count_data_items(files_test)\nsteps = cnt_test / (CFG['batch_size'] * REPLICAS) * CFG['tta_steps']\nds_testAug = get_dataset(files_test, augment=True, repeat=True, \n                         labeled=False, return_image_names=False, is_test=True)\n\npredictions = model_with_metadata.predict(ds_testAug, verbose=1, steps=steps)\n\n# Use test time image augmentation and take the mean of all values\npreds = np.stack(predictions)\npreds = preds[:,:cnt_test* CFG['tta_steps']]\npreds = preds[:df_test.shape[0]*CFG['tta_steps']]\npreds = np.stack(np.split(preds, CFG['tta_steps']),axis=1)\npreds = np.mean(preds, axis=1)\nfinal_predictions = preds.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"id":"wM_46JcUc1YT"},"cell_type":"markdown","source":"## Sort image names and create submission file"},{"metadata":{"id":"1GWgt4Xnc1YU"},"cell_type":"markdown","source":"The submission ist sorted by image_name, but the dataset yielded a different order. Traverse the test dataset once again and capture the image_names. Then join this list of image_names with the predictions and sort by image_name."},{"metadata":{"trusted":true,"id":"nxmUvbAMc1YU"},"cell_type":"code","source":"ds = get_dataset(files_test, augment=False, repeat=False, \n                 labeled=False, return_image_names=True)\n\nimage_names = np.array([img_name[0].numpy().decode(\"utf-8\") \n                        for example, img_name in iter(ds.unbatch())])\n\nsubmission = pd.DataFrame(dict(\n    image_name=image_names,\n    target=final_predictions))\n\nsubmission = submission.sort_values('image_name')\nfilename=f'{CFG[\"model_name\"]}-{CFG[\"FC_size\"]}-{CFG[\"epochs\"]}epc-{CFG[\"label_smooth_fac\"]}_lbsf.csv'\nsubmission.to_csv(filename, index=False)\n\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}